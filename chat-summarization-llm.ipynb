{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-19T00:56:47.556954Z","iopub.execute_input":"2024-11-19T00:56:47.557304Z","iopub.status.idle":"2024-11-19T00:56:47.563128Z","shell.execute_reply.started":"2024-11-19T00:56:47.557272Z","shell.execute_reply":"2024-11-19T00:56:47.562110Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"!nvidia-smi","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T00:56:47.567473Z","iopub.execute_input":"2024-11-19T00:56:47.567787Z","iopub.status.idle":"2024-11-19T00:56:48.733506Z","shell.execute_reply.started":"2024-11-19T00:56:47.567738Z","shell.execute_reply":"2024-11-19T00:56:48.732262Z"}},"outputs":[{"name":"stdout","text":"Tue Nov 19 00:56:48 2024       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla P100-PCIE-16GB           Off |   00000000:00:04.0 Off |                    0 |\n| N/A   38C    P0             27W /  250W |       0MiB /  16384MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"!pip install transformers[sentencepiece] datasets sacrebleu rouge_score py7zr -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T00:56:48.735106Z","iopub.execute_input":"2024-11-19T00:56:48.735476Z","iopub.status.idle":"2024-11-19T00:56:58.405839Z","shell.execute_reply.started":"2024-11-19T00:56:48.735431Z","shell.execute_reply":"2024-11-19T00:56:58.404691Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"!pip install --upgrade accelerate\n!pip uninstall -y transformers accelerate\n!pip install transformers accelerate","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import pipeline, set_seed\nfrom datasets import load_dataset, load_from_disk\nimport matplotlib.pyplot as plt\nfrom datasets import load_dataset\nimport pandas as pd\nfrom datasets import load_dataset\n\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\nimport nltk\nfrom nltk.tokenize import sent_tokenize\n\nfrom tqdm import tqdm\nimport torch\n\nnltk.download(\"punkt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T00:56:58.407594Z","iopub.execute_input":"2024-11-19T00:56:58.407934Z","iopub.status.idle":"2024-11-19T00:57:19.307223Z","shell.execute_reply.started":"2024-11-19T00:56:58.407902Z","shell.execute_reply":"2024-11-19T00:57:19.306167Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T00:57:19.308653Z","iopub.execute_input":"2024-11-19T00:57:19.309262Z","iopub.status.idle":"2024-11-19T00:57:19.346982Z","shell.execute_reply.started":"2024-11-19T00:57:19.309230Z","shell.execute_reply":"2024-11-19T00:57:19.345893Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM, AutoTokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T00:57:19.352879Z","iopub.execute_input":"2024-11-19T00:57:19.353315Z","iopub.status.idle":"2024-11-19T00:57:19.372186Z","shell.execute_reply.started":"2024-11-19T00:57:19.353283Z","shell.execute_reply":"2024-11-19T00:57:19.371188Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Load the tokenizer for the \"google/pegasus-cnn_dailymail\" model, used for text summarization tasks.\ntokenizer = AutoTokenizer.from_pretrained(\"google/pegasus-cnn_dailymail\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T00:57:19.373300Z","iopub.execute_input":"2024-11-19T00:57:19.373623Z","iopub.status.idle":"2024-11-19T00:57:21.359546Z","shell.execute_reply.started":"2024-11-19T00:57:19.373593Z","shell.execute_reply":"2024-11-19T00:57:21.358436Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/88.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"639a6516def5430384f36a574e86087a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.12k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e59e518181ad4cd6b40f576e35462a28"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/1.91M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1686a291115b438ba3c1cc526410f5b9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb41da3e24984291a2fdf3352a225e30"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Load the PEGASUS model for sequence-to-sequence tasks, optimized for text summarization, and move it to the specified device (e.g., GPU or CPU).\nmodel_pegasus = AutoModelForSeq2SeqLM.from_pretrained(\"google/pegasus-cnn_dailymail\").to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T00:57:21.361040Z","iopub.execute_input":"2024-11-19T00:57:21.361475Z","iopub.status.idle":"2024-11-19T00:57:43.645386Z","shell.execute_reply.started":"2024-11-19T00:57:21.361428Z","shell.execute_reply":"2024-11-19T00:57:43.644570Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/2.28G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5dc2ba2ee5224d93a549dec032b83348"}},"metadata":{}},{"name":"stderr","text":"Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/280 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85eedf01665f4795be868b0196a8ae7d"}},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"# Load the SAMSum dataset, which is commonly used for dialogue summarization tasks.\ndataset_samsum = load_dataset(\"samsum\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T00:57:43.646581Z","iopub.execute_input":"2024-11-19T00:57:43.646932Z","iopub.status.idle":"2024-11-19T00:57:52.893085Z","shell.execute_reply.started":"2024-11-19T00:57:43.646901Z","shell.execute_reply":"2024-11-19T00:57:52.892047Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"samsum.py:   0%|          | 0.00/3.36k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b151e756d41b4bdfa295043ace64190e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/7.04k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"832b2b8f79584f6e8b53866f51f9c226"}},"metadata":{}},{"output_type":"stream","name":"stdin","text":"The repository for samsum contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/samsum.\nYou can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n\nDo you wish to run the custom code? [y/N]  y\n"},{"output_type":"display_data","data":{"text/plain":"corpus.7z:   0%|          | 0.00/2.94M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b003e5ec36a40a199c37ceeaf002ad3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/14732 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11e9e050b02b46c9b72060e263e4d7d8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/819 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"941aec038fce4926928c7cfda41c3fa7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/818 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cee3ba6e7e424dcdbe46f22083bbf78a"}},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"dataset_samsum","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T00:57:52.894450Z","iopub.execute_input":"2024-11-19T00:57:52.894852Z","iopub.status.idle":"2024-11-19T00:57:52.901920Z","shell.execute_reply.started":"2024-11-19T00:57:52.894806Z","shell.execute_reply":"2024-11-19T00:57:52.900838Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'dialogue', 'summary'],\n        num_rows: 14732\n    })\n    test: Dataset({\n        features: ['id', 'dialogue', 'summary'],\n        num_rows: 819\n    })\n    validation: Dataset({\n        features: ['id', 'dialogue', 'summary'],\n        num_rows: 818\n    })\n})"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"dataset_samsum[\"train\"][0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T00:57:52.903288Z","iopub.execute_input":"2024-11-19T00:57:52.903613Z","iopub.status.idle":"2024-11-19T00:57:52.919395Z","shell.execute_reply.started":"2024-11-19T00:57:52.903582Z","shell.execute_reply":"2024-11-19T00:57:52.918329Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"{'id': '13818513',\n 'dialogue': \"Amanda: I baked  cookies. Do you want some?\\r\\nJerry: Sure!\\r\\nAmanda: I'll bring you tomorrow :-)\",\n 'summary': 'Amanda baked cookies and will bring Jerry some tomorrow.'}"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"def convert_examples_to_features(batch):\n    \"\"\"\n    Converts a batch of examples from the SAMSum dataset into features suitable for model training.\n    \"\"\"\n    # Tokenize the 'dialogue' field with padding to the maximum length and truncation.\n    input_encodings = tokenizer(batch['dialogue'], padding=\"max_length\", truncation=True)\n\n    # Tokenize the 'summary' field with a maximum length of 128 and truncation.\n    target_encodings = tokenizer(batch['summary'], max_length=128, truncation=True)\n\n    return {\n        'input_ids': input_encodings['input_ids'],       # Token IDs for the input dialogues\n        'attention_mask': input_encodings['attention_mask'],  # Attention mask for the input dialogues\n        'labels': target_encodings['input_ids']         # Token IDs for the target summaries\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T00:57:52.920611Z","iopub.execute_input":"2024-11-19T00:57:52.921075Z","iopub.status.idle":"2024-11-19T00:57:52.932510Z","shell.execute_reply.started":"2024-11-19T00:57:52.921023Z","shell.execute_reply":"2024-11-19T00:57:52.931555Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# Apply the `convert_examples_to_features` function to the SAMSum dataset to tokenize the dialogues and summaries, \n# transforming the dataset into a format suitable for model training. The 'batched=True' argument allows the function \n# to process multiple examples at once for efficiency.\ndataset_samsum = dataset_samsum.map(convert_examples_to_features, batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T00:57:52.933651Z","iopub.execute_input":"2024-11-19T00:57:52.934039Z","iopub.status.idle":"2024-11-19T00:58:04.010782Z","shell.execute_reply.started":"2024-11-19T00:57:52.933951Z","shell.execute_reply":"2024-11-19T00:58:04.009845Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/14732 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40361598a414432b97bc5c753b192530"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/819 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d15f1f2610f44c1aae539146bcaf04e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/818 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3b139cb50d1497c963bdede1be66e13"}},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"dataset_samsum","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T00:58:04.011856Z","iopub.execute_input":"2024-11-19T00:58:04.012171Z","iopub.status.idle":"2024-11-19T00:58:04.018560Z","shell.execute_reply.started":"2024-11-19T00:58:04.012142Z","shell.execute_reply":"2024-11-19T00:58:04.017430Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'dialogue', 'summary', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 14732\n    })\n    test: Dataset({\n        features: ['id', 'dialogue', 'summary', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 819\n    })\n    validation: Dataset({\n        features: ['id', 'dialogue', 'summary', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 818\n    })\n})"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"from transformers import DataCollatorForSeq2Seq\n\n# Create a data collator for sequence-to-sequence tasks that handles dynamic padding and prepares batches for the model.\n# It uses the tokenizer and model (PEGASUS) to properly format the input sequences and labels during training.\nseq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model_pegasus)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T00:58:04.020089Z","iopub.execute_input":"2024-11-19T00:58:04.020775Z","iopub.status.idle":"2024-11-19T00:58:04.034481Z","shell.execute_reply.started":"2024-11-19T00:58:04.020725Z","shell.execute_reply":"2024-11-19T00:58:04.033452Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\n\n# Set up the training arguments for the Trainer, specifying various hyperparameters and training configurations.\n# This includes the output directory, number of epochs, batch sizes, weight decay, and logging/evaluation settings.\ntrainer_args = TrainingArguments(\n    output_dir='/kaggle/working/',          # Directory where model checkpoints and logs will be saved\n    num_train_epochs=1,                  # Number of training epochs\n    warmup_steps=500,                    # Number of steps for learning rate warmup\n    per_device_train_batch_size=1,       # Batch size for training\n    per_device_eval_batch_size=1,        # Batch size for evaluation\n    weight_decay=0.01,                   # Weight decay to avoid overfitting\n    logging_steps=10,                    # Frequency of logging steps\n    evaluation_strategy='steps',         # Evaluate the model at each evaluation step\n    eval_steps=500,                      # Frequency of evaluation steps\n    save_steps=1e6,                      # Save model checkpoint after every 1 million steps\n    gradient_accumulation_steps=16       # Number of steps to accumulate gradients before performing a backward pass\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T00:58:04.035708Z","iopub.execute_input":"2024-11-19T00:58:04.036073Z","iopub.status.idle":"2024-11-19T00:58:05.179200Z","shell.execute_reply.started":"2024-11-19T00:58:04.036037Z","shell.execute_reply":"2024-11-19T00:58:05.178108Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# Initialize the Trainer with the model, training arguments, tokenizer, data collator, and datasets for training and evaluation.\n# This sets up the environment for training and evaluating the PEGASUS model on the SAMSum dataset.\ntrainer = Trainer(\n    model=model_pegasus,                     # The PEGASUS model to be trained\n    args=trainer_args,                       # The training arguments specifying hyperparameters and settings\n    tokenizer=tokenizer,                     # The tokenizer to process text data\n    data_collator=seq2seq_data_collator,     # Data collator for padding and formatting batches\n    train_dataset=dataset_samsum[\"train\"], # The test dataset to use for training\n    eval_dataset=dataset_samsum[\"test\"]  # The validation dataset to use for evaluation during training\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T00:58:05.180625Z","iopub.execute_input":"2024-11-19T00:58:05.180949Z","iopub.status.idle":"2024-11-19T00:58:05.967142Z","shell.execute_reply.started":"2024-11-19T00:58:05.180918Z","shell.execute_reply":"2024-11-19T00:58:05.966395Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T00:58:05.968420Z","iopub.execute_input":"2024-11-19T00:58:05.969097Z","iopub.status.idle":"2024-11-19T02:33:41.886923Z","shell.execute_reply.started":"2024-11-19T00:58:05.969047Z","shell.execute_reply":"2024-11-19T02:33:41.886081Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  Â·Â·Â·Â·Â·Â·Â·Â·\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113969011113214, max=1.0â€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b09373d893e4452ebfb02b040d913ae2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241119_005847-9cm91njy</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/paritkansal121-harcourt-butler-technical-university/huggingface/runs/9cm91njy' target=\"_blank\">/kaggle/working/</a></strong> to <a href='https://wandb.ai/paritkansal121-harcourt-butler-technical-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/paritkansal121-harcourt-butler-technical-university/huggingface' target=\"_blank\">https://wandb.ai/paritkansal121-harcourt-butler-technical-university/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/paritkansal121-harcourt-butler-technical-university/huggingface/runs/9cm91njy' target=\"_blank\">https://wandb.ai/paritkansal121-harcourt-butler-technical-university/huggingface/runs/9cm91njy</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='920' max='920' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [920/920 1:34:44, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>1.658000</td>\n      <td>1.511652</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:2618: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 128, 'min_length': 32, 'num_beams': 8, 'length_penalty': 0.8}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=920, training_loss=1.8314198929330576, metrics={'train_runtime': 5734.8577, 'train_samples_per_second': 2.569, 'train_steps_per_second': 0.16, 'total_flos': 4.253291617714176e+16, 'train_loss': 1.8314198929330576, 'epoch': 0.9991854466467553})"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"## Save model\nmodel_pegasus.save_pretrained(\"/kaggle/working/\")\n## Save tokenizer\ntokenizer.save_pretrained(\"/kaggle/working/\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T02:33:41.888094Z","iopub.execute_input":"2024-11-19T02:33:41.888400Z","iopub.status.idle":"2024-11-19T02:33:50.390333Z","shell.execute_reply.started":"2024-11-19T02:33:41.888362Z","shell.execute_reply":"2024-11-19T02:33:50.389354Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"('/kaggle/working/tokenizer_config.json',\n '/kaggle/working/special_tokens_map.json',\n '/kaggle/working/spiece.model',\n '/kaggle/working/added_tokens.json',\n '/kaggle/working/tokenizer.json')"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\n# Load the fine-tuned model and tokenizer from the saved checkpoint\nmodel_ckpt = \"/kaggle/working/\"  # Path where the model was saved after fine-tuning\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)  # Load the tokenizer, which converts text to token IDs\nmodel_pegasus = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)  # Load the fine-tuned model and move it to the specified device (GPU or CPU)\n\n# Function to generate a summary for the input dialogue text\ndef generate_summary(input_text):\n    \"\"\"\n    Takes input_text (dialogue) as input and generates a summary using the pre-trained Pegasus model.\n    It tokenizes the input text, feeds it into the model, and decodes the output tokens into a summary.\n    \"\"\"\n\n    # Tokenize the input text and prepare it for the model\n    inputs = tokenizer(input_text, \n                       return_tensors=\"pt\",  # Return tokenized inputs as PyTorch tensors, which is the format the model expects\n                       padding=\"max_length\",  # Pad input text to the maximum length (1024 tokens). Ensures uniform input size across batches.\n                       truncation=True,  # Truncate input if it exceeds max_length to fit the model's input size.\n                      ).to(device)  # Move tensors to the correct device (GPU or CPU).\n    \n    # Generate the summary from the model\n    summary_ids = model_pegasus.generate(\n        inputs['input_ids'],  # The tokenized input text is passed to the model\n        num_beams=4,  # Use beam search with 4 beams, which generates multiple sequences and selects the best one. Helps in improving summary quality.\n        max_length=128,  # The length of the generated summary. The summary will be capped at 128 tokens.\n        early_stopping=True)  # Stop the generation process as soon as the model generates an end-of-sequence token (avoids generating unnecessary tokens)\n    \n    # Decode the tokenized summary back into readable text\n    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)  # Converts the generated token IDs back to human-readable text, skipping special tokens like <eos>, <pad>, etc.\n    \n    return summary  # Return the generated summary\n\n# Example test dialogue to test the model's summarization capabilities\ntest_dialogue = dataset_samsum[\"train\"][0][\"dialogue\"]\n\n# Call the generate_summary function and print the result\nsummary = generate_summary(test_dialogue)\nprint(\"Generated Summary:\", summary)  # Output the generated summary of the input dialogue","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T02:55:10.265377Z","iopub.execute_input":"2024-11-19T02:55:10.266209Z","iopub.status.idle":"2024-11-19T02:55:17.000877Z","shell.execute_reply.started":"2024-11-19T02:55:10.266154Z","shell.execute_reply":"2024-11-19T02:55:16.999482Z"}},"outputs":[{"name":"stdout","text":"Generated Summary: Amanda baked cookies. She will bring them tomorrow. Jerry will buy them. Amanda will bring them to him. Jerry will bring them to Amanda. Amanda will bring them tomorrow.\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"dataset_samsum[\"train\"][0][\"dialogue\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T02:54:54.099222Z","iopub.execute_input":"2024-11-19T02:54:54.099623Z","iopub.status.idle":"2024-11-19T02:54:54.107396Z","shell.execute_reply.started":"2024-11-19T02:54:54.099592Z","shell.execute_reply":"2024-11-19T02:54:54.106518Z"}},"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"\"Amanda: I baked  cookies. Do you want some?\\r\\nJerry: Sure!\\r\\nAmanda: I'll bring you tomorrow :-)\""},"metadata":{}}],"execution_count":37}]}